# ğŸ“Š User Engagement & Marketing ETL Pipeline

A modular ETL pipeline project that simulates real-world data engineering tasks by combining **user behavior data** (gameplay logs) and **marketing campaign data** to generate actionable insights â€” all fully orchestrated and containerized using Docker.

---

## ğŸš€ Features

- ğŸ›  Extracts user event data and marketing campaigns from **JSON files** (simulating APIs or streaming data)
- ğŸ”„ Transforms, aggregates, and joins datasets using **Pandas**
- ğŸ”— Merges campaign and gameplay data on `user_id` to create a unified engagement view
- ğŸ’¾ Loads the final dataset into a **PostgreSQL** database (simulating cloud warehouse like Redshift)
- ğŸ“… Scheduled and orchestrated with **Apache Airflow DAGs**
- ğŸ“¦ Fully containerized using **Docker Compose** (Airflow + PostgreSQL)
- ğŸ–¥ Visualized with an interactive **Streamlit dashboard**
- âœ… Runs seamlessly inside **GitHub Codespaces**


## ğŸ“¸ Dashboard Preview


<div align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/marketing-analytics-pipeline/streamlit.png" alt="Streamlit Dashboard 1" width="45%"/>
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/marketing-analytics-pipeline/streamlit2.png" alt="Streamlit Dashboard 2" width="45%"/>
</div>
---

## ğŸ§  Project Architecture
```
data sources
â†“
[ Extract ]
â†“
[ Transform ]
â†“
[ Load to PostgreSQL ]
â†“
[ Ready for Analytics / BI Dashboards ]
```
---

## ğŸ§± Tech Stack
```
| Stage        | Tools/Technologies                  |
|--------------|-------------------------------------|
| Extract      | Python, Pandas                      |
| Transform    | Pandas, SQL                         |
| Load         | SQLAlchemy, PostgreSQL              |
| Orchestration| Apache Airflow                      |
| Containerize | Docker, docker-compose              |
| Dev Env      | GitHub Codespaces, Virtualenv       |
```
---

## ğŸ“‚ Project Structure
```
marketing-analytics-pipeline/
â”‚
â”œâ”€â”€ extract/                  # Extraction scripts
â”‚   â”œâ”€â”€ extract_game_events.py
â”‚   â””â”€â”€ extract_campaigns.py
â”‚
â”œâ”€â”€ transform/                # Data transformation logic
â”‚   â””â”€â”€ transform_data.py
â”‚
â”œâ”€â”€ load/                     # PostgreSQL loader
â”‚   â””â”€â”€ load_to_postgres.py
â”‚
â”œâ”€â”€ dags/                     # Airflow DAGs
â”‚   â””â”€â”€ marketing_etl_dag.py
â”‚
â”œâ”€â”€ dashboard/                # Streamlit dashboard app
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ airflow/                  # Airflow docker-compose setup
â”‚   â””â”€â”€ docker-compose.yaml
â”‚
â”œâ”€â”€ data/                     # Sample CSV data
â”‚   â”œâ”€â”€ game_events.csv
â”‚   â””â”€â”€ campaigns.csv
â”‚
â”œâ”€â”€ run_pipeline.py           # CLI runner for ETL
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md
```
---

## âš™ï¸ Setup Instructions

### 1. ğŸ”§ Clone the repository

```bash
git clone https://github.com/YOUR_USERNAME/marketing-analytics-pipeline.git
cd marketing-analytics-pipeline
```
### 2. ğŸ³ Start PostgreSQL with Docker
```
docker-compose -f docker/docker-compose.yml up -d
```
### 3. ğŸ Create & activate virtual environment
```
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 5. Run the ETL Pipeline Manually (Optional)
```
python run_pipeline.py
```

### 6. Run the Streamlit Dashboard
```
cd dashboard
streamlit run app.py
```

## ğŸ”„ Run Apache Airflow Locally
# Navigate to Airflow folder:
```
cd airflow
```
# First-Time Initialization (run once):
```
docker-compose up --build
```
This automatically runs airflow db init and creates the admin user.

# Run Airflow services:
```
docker-compose up webserver scheduler
```








### âœ… Sample Use Case
```
This project answers questions like:
	â€¢	Which campaigns lead to high user engagement?
	â€¢	Whatâ€™s the average playtime or purchase value per campaign?
	â€¢	Are paid campaigns outperforming organic channels?
```

ğŸ“Š Sample Output After Transformation

This will help answer questions like:
	â€¢	Which campaigns led to higher revenue or playtime?
	â€¢	Which users were engaged but did not click ads?

	
	1. pip install -r requirements.txt
	2. docker-compose -f docker/docker-compose.yml up -d
	

	these are in requirements:
	3. pip install streamlit
	4. streamlit run app.py

## how to run Streamlit:

	streamlit run dashboard/app.py

##  Initialize the Airflow database (first-time only)
docker-compose up airflow-init
docker-compose up webserver scheduler

## Start Airflow services
docker-compose up

```
cd airflow
docker-compose up airflow-init  # No need for this , becouse it include the docker Run once to initialize
docker-compose up webserver scheduler #bring up the main services:
docker-compose up               # Then start Airflow
```

ğŸŒ Access Airflow UI
Visit:
http://localhost:8080

Login:
	â€¢	Username: airflow
	â€¢	Password: airflow

Make sure Docker is installed and running on your system. (for PostgreSQL you should install docker)

- [Install Docker](https://docs.docker.com/get-docker/)
- Then run:
```bash
docker-compose -f docker/docker-compose.yml up -d


