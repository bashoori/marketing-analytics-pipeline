# ğŸ§  Marketing Analytics ETL Pipeline with Apache Airflow

This project implements a modular ETL pipeline using **Apache Airflow** to extract, transform, and load marketing data from various sources. It uses custom Python scripts organized in `extract/`, `transform/`, and `load/` directories, and runs via Airflow using Docker Compose.


## âš™ï¸ Technologies

- **Apache Airflow 2.7.1**
- **Python 3.10+**
- **PostgreSQL (Airflow + Marketing DB)**
- **Docker + Docker Compose**


## ğŸ“¸ Dashboard Preview

<div align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/marketing-analytics-pipeline/streamlit.png" alt="Streamlit Dashboard 1" width="45%"/>
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/marketing-analytics-pipeline/streamlit2.png" alt="Streamlit Dashboard 2" width="45%"/>
  <br/>
  <a href="https://github.com/bashoori/repo/blob/master/marketing-analytics-pipeline/airflow.png" target="_blank">
    <img src="https://raw.githubusercontent.com/bashoori/repo/master/marketing-analytics-pipeline/airflow.png" alt="Airflow DAG" width="45%"/>
  </a>
  <a href="https://github.com/bashoori/repo/blob/master/marketing-analytics-pipeline/airflow2.png" target="_blank">
    <img src="https://raw.githubusercontent.com/bashoori/repo/master/marketing-analytics-pipeline/airflow2.png" alt="Airflow DAG Status" width="45%"/>
  </a>
</div>


## ğŸ“‚ Project Structure
```
marketing-analytics-pipeline/
â”œâ”€â”€ dags/                     # Airflow DAGs
â”œâ”€â”€ data/                     # Raw and staging data (JSON, CSV)
â”œâ”€â”€ etl/                      # Modular ETL code
â”‚   â”œâ”€â”€ extract/              # Data extraction scripts
â”‚   â”œâ”€â”€ transform/            # Data cleaning & transformation
â”‚   â””â”€â”€ load/                 # Load to PostgreSQL
â”œâ”€â”€ dashboard/                # Streamlit dashboard app
â”œâ”€â”€ scripts/                  # Utility scripts (run_pipeline.py, test_json.py, etc.)
â”œâ”€â”€ logs/                     # Airflow logs (gitignored)
â”œâ”€â”€ plugins/                  # Airflow plugins (if any)
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ docker-compose.yaml       # Container setup
â”œâ”€â”€ .env                      # Environment variables (excluded from Git)
â””â”€â”€ airflow.cfg               # Airflow config (excluded from Git)
```

---

## ğŸš€ Features

- Modular ETL: Cleanly separated Extract, Transform, Load steps

- Airflow 2.7.1: DAG-based orchestration with retries, scheduling, and logging

- PostgreSQL: Used for both Airflow metadata and final marketing DB

- Docker Compose: One-command containerized deployment

- Streamlit Dashboard: Real-time visualization of key marketing KPIs

---

ğŸŒ Streamlit Dashboard Preview

The dashboard includes:

- ğŸ“Š Total Revenue, Average Order Value

- ğŸ“‰ Conversion Rate & Click-Through Rate

- ğŸ“ Sales Distribution by Region (Map)

- ğŸ•’ Filters by Date & Product Category


###  Run locally:
```
cd dashboard
streamlit run dashboard.py
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/your-username/marketing-analytics-pipeline.git
cd marketing-analytics-pipeline
```

### 2. Add Environment Variables

Create a .env file:
```
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow
AIRFLOW_DB_NAME=airflow

MARKETING_DB_USER=marketer
MARKETING_DB_PASSWORD=marketer
MARKETING_DB_NAME=marketing
```
### 3. Start Airflow + PostgreSQL Services
```
docker-compose down --volumes
docker-compose up --build 
```
### 4. Access Airflow UI

Visit: http://localhost:8080
Login: airflow / airflow

Enable the DAG named marketing_etl_pipeline.

## ğŸ§ª DAG Logic
	â€¢	extract_game_events: Pulls raw game events
	â€¢	extract_campaign_data: Pulls ad campaign data
	â€¢	transform_data: Joins & cleans the extracted data
	â€¢	load_data: Loads final data into a PostgreSQL table

Dependency Flow:
```
[extract_game_events, extract_campaign_data] â†’ transform_data â†’ load_data
```

## ğŸ§¼ Troubleshooting

â€œModuleNotFoundError: No module named â€˜extractâ€™â€

Make sure the full project is mounted in Docker:

In docker-compose.yaml:
```
volumes:
  - .:/opt/airflow  # âœ… Mount the entire project root
```

Also, the DAG includes:
```
sys.path.append(os.path.abspath(os.path.dirname(os.path.dirname(__file__))))
```

## ğŸ“ˆ Future Improvements
	â€¢	Add unit tests
	â€¢	Enable logging and monitoring
	â€¢	Add email alerts for task failures
	â€¢	Use Docker Secrets for credentials
	â€¢	Deploy to cloud (e.g. AWS, GCP Composer)


## ğŸ‘©â€ğŸ’» Author

Bita Ashoori
Data Engineer 